{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2993857,"sourceType":"datasetVersion","datasetId":1834494},{"sourceId":9876308,"sourceType":"datasetVersion","datasetId":6063432}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.utils import resample\n\n# Define base directories for Sessions 1 and 2\nsession1_directory = '/kaggle/input/iemocapfullrelease/IEMOCAP_full_release/Session1/sentences/wav/'  # Adjust if necessary\nsession2_directory = '/kaggle/input/iemocapfullrelease/IEMOCAP_full_release/Session2/sentences/wav/'  # Adjust as needed\ncsv_file_path = '/kaggle/input/iemocap-csv/iemocap_full_dataset.csv'  # Adjust to match the uploaded CSV file path\n\n# Load the CSV file\ndata = pd.read_csv(csv_file_path)\n\n# Update the paths in the 'path' column to match Kaggle's file structure for both sessions\ndata['path'] = data['path'].apply(lambda x: x.replace('Session1/sentences/wav/', session1_directory))\ndata['path'] = data['path'].apply(lambda x: x.replace('Session2/sentences/wav/', session2_directory))\n\n# Filter data for valid emotions (not 'xxx') and files from Session 1 or Session 2\nfiltered_data = data[(data['emotion'] != 'xxx') & \n                     (data['path'].str.contains(session1_directory) | data['path'].str.contains(session2_directory))]\n\n# Encode emotion labels\nle = LabelEncoder()\nfiltered_data['emotion_label'] = le.fit_transform(filtered_data['emotion'])\n\n# Display the total number of labeled files\ntotal_labeled_files = filtered_data.shape[0]\nprint(f\"Total number of labeled files across Session 1 and 2: {total_labeled_files}\")\n\n# Count the number of files for each emotion\nemotion_counts = filtered_data['emotion'].value_counts()\n\n# Balance the dataset\nbalanced_data = pd.DataFrame()  # Initialize an empty DataFrame for balanced data\nmax_count = emotion_counts.max()  # Find the maximum count of any emotion category\n\n# Resample each emotion to match the maximum count\nfor emotion in emotion_counts.index:\n    subset = filtered_data[filtered_data['emotion'] == emotion]\n    if len(subset) < max_count:\n        # Upsample to match the majority class count\n        subset = resample(subset, replace=True, n_samples=max_count, random_state=42)\n    balanced_data = pd.concat([balanced_data, subset])\n\n# Verify balancing by counting the files per emotion in the balanced dataset\nbalanced_emotion_counts = balanced_data['emotion'].value_counts()\n\n# Prepare data for side-by-side bar plot\nemotion_data = pd.DataFrame({\n    'Emotion': emotion_counts.index,\n    'Before Balancing': emotion_counts.values,\n    'After Balancing': balanced_emotion_counts[emotion_counts.index].values  # Ensure order matches 'Before Balancing'\n})\n\n# Melt the DataFrame for seaborn compatibility\nemotion_data_melted = emotion_data.melt(id_vars='Emotion', \n                                        value_vars=['Before Balancing', 'After Balancing'], \n                                        var_name='Condition', \n                                        value_name='Count')\n\n# Plot side-by-side bar chart\nplt.figure(figsize=(12, 6))\nsns.barplot(data=emotion_data_melted, x='Emotion', y='Count', hue='Condition', palette='viridis')\nplt.title('Number of Files for Each Emotion Before and After Balancing (Session 1 & 2)')\nplt.xlabel('Emotions')\nplt.ylabel('Count')\nplt.xticks(rotation=45)\nplt.legend(title='Condition')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T19:05:18.365216Z","iopub.execute_input":"2024-12-09T19:05:18.365606Z","iopub.status.idle":"2024-12-09T19:05:18.726081Z","shell.execute_reply.started":"2024-12-09T19:05:18.365574Z","shell.execute_reply":"2024-12-09T19:05:18.725298Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/1949593283.py:25: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  filtered_data['emotion_label'] = le.fit_transform(filtered_data['emotion'])\n","output_type":"stream"},{"name":"stdout","text":"Total number of labeled files across Session 1 and 2: 2779\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1200x600 with 1 Axes>","image/png":"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"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"from tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, LSTM, Bidirectional, Conv2D, BatchNormalization, MaxPooling2D, Flatten, Dense, Dropout, Concatenate, Reshape\n\n# Define the model function\ndef create_model(input_shape, num_classes):\n    input_layer = Input(shape=input_shape)\n    \n    # BiLSTM branch\n    bilstm_input = Reshape((13, 1))(input_layer)\n    bilstm1 = Bidirectional(LSTM(64, return_sequences=True))(bilstm_input)\n    bilstm2 = Bidirectional(LSTM(64))(bilstm1)\n\n    # Conv2D branch\n    conv_input = Reshape((13, 1, 1))(input_layer)\n    conv2d = Conv2D(32, kernel_size=(3, 1), dilation_rate=(2, 1), activation='relu', padding='same')(conv_input)\n    conv2d = BatchNormalization()(conv2d)\n    conv2d = MaxPooling2D(pool_size=(2, 1))(conv2d)\n\n    conv2d = Conv2D(64, kernel_size=(3, 1), dilation_rate=(2, 1), activation='relu', padding='same')(conv2d)\n    conv2d = BatchNormalization()(conv2d)\n    conv2d = MaxPooling2D(pool_size=(2, 1))(conv2d)\n\n    conv2d = Conv2D(128, kernel_size=(3, 1), dilation_rate=(2, 1), activation='relu', padding='same')(conv2d)\n    conv2d = BatchNormalization()(conv2d)\n    conv2d = MaxPooling2D(pool_size=(2, 1))(conv2d)\n\n    # Flatten and dense layers for Conv2D branch\n    conv_flat = Flatten()(conv2d)\n    dense1 = Dense(512, activation='relu')(conv_flat)\n    dropout4 = Dropout(0.2)(dense1)\n    dense2 = Dense(128, activation='relu')(dropout4)\n    dropout5 = Dropout(0.2)(dense2)\n\n    # Concatenate BiLSTM and Conv2D branches\n    concatenated = Concatenate()([bilstm2, dropout5])\n    dense = Dense(64, activation='relu')(concatenated)\n    output_layer = Dense(num_classes, activation='softmax')(dense)\n\n    # Model definition and compilation\n    model = Model(inputs=input_layer, outputs=output_layer)\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    \n    return model\n\n# Create the model\nmodel = create_model((13, 1), 10)  # Adjust the number of classes as needed\n\n# Print the model summary\nmodel.summary()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T16:38:04.006755Z","iopub.execute_input":"2024-12-30T16:38:04.007011Z","iopub.status.idle":"2024-12-30T16:38:17.105530Z","shell.execute_reply.started":"2024-12-30T16:38:04.006984Z","shell.execute_reply":"2024-12-30T16:38:17.104654Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_1\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\n\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m\n\n input_layer          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m1\u001b[0m)               \u001b[38;5;34m0\u001b[0m  -                 \n (\u001b[38;5;33mInputLayer\u001b[0m)                                                          \n\n reshape_1 (\u001b[38;5;33mReshape\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m)            \u001b[38;5;34m0\u001b[0m  input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n\n conv2d (\u001b[38;5;33mConv2D\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m32\u001b[0m)         \u001b[38;5;34m128\u001b[0m  reshape_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n batch_normalization  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m32\u001b[0m)         \u001b[38;5;34m128\u001b[0m  conv2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n (\u001b[38;5;33mBatchNormalizatio\u001b[0m                                                   \n\n max_pooling2d        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m32\u001b[0m)            \u001b[38;5;34m0\u001b[0m  batch_normalizat \n (\u001b[38;5;33mMaxPooling2D\u001b[0m)                                                        \n\n conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)        \u001b[38;5;34m6,208\u001b[0m  max_pooling2d[\u001b[38;5;34m0\u001b[0m] \n\n batch_normalizatio  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)          \u001b[38;5;34m256\u001b[0m  conv2d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mBatchNormalizatio\u001b[0m                                                   \n\n max_pooling2d_1      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  batch_normalizat \n (\u001b[38;5;33mMaxPooling2D\u001b[0m)                                                        \n\n conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m)      \u001b[38;5;34m24,704\u001b[0m  max_pooling2d_1[\u001b[38;5;34m\u001b[0m \n\n batch_normalizatio  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m)         \u001b[38;5;34m512\u001b[0m  conv2d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mBatchNormalizatio\u001b[0m                                                   \n\n max_pooling2d_2      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m)           \u001b[38;5;34m0\u001b[0m  batch_normalizat \n (\u001b[38;5;33mMaxPooling2D\u001b[0m)                                                        \n\n flatten (\u001b[38;5;33mFlatten\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 \u001b[38;5;34m0\u001b[0m  max_pooling2d_2[\u001b[38;5;34m\u001b[0m \n\n dense (\u001b[38;5;33mDense\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            \u001b[38;5;34m66,048\u001b[0m  flatten[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n\n reshape (\u001b[38;5;33mReshape\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m1\u001b[0m)               \u001b[38;5;34m0\u001b[0m  input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n\n dropout (\u001b[38;5;33mDropout\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 \u001b[38;5;34m0\u001b[0m  dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n bidirectional        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m128\u001b[0m)        \u001b[38;5;34m33,792\u001b[0m  reshape[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n (\u001b[38;5;33mBidirectional\u001b[0m)                                                       \n\n dense_1 (\u001b[38;5;33mDense\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            \u001b[38;5;34m65,664\u001b[0m  dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n\n bidirectional_1      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            \u001b[38;5;34m98,816\u001b[0m  bidirectional[\u001b[38;5;34m0\u001b[0m] \n (\u001b[38;5;33mBidirectional\u001b[0m)                                                       \n\n dropout_1 (\u001b[38;5;33mDropout\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 \u001b[38;5;34m0\u001b[0m  dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n\n concatenate          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 \u001b[38;5;34m0\u001b[0m  bidirectional_1[\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mConcatenate\u001b[0m)                                       dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n dense_2 (\u001b[38;5;33mDense\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             \u001b[38;5;34m16,448\u001b[0m  concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n\n dense_3 (\u001b[38;5;33mDense\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                \u001b[38;5;34m650\u001b[0m  dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n<span style=\"font-weight: bold\"> Layer (type)        </span><span style=\"font-weight: bold\"> Output Shape      </span><span style=\"font-weight: bold\">    Param # </span><span style=\"font-weight: bold\"> Connected to      </span>\n\n input_layer          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                          \n\n reshape_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n\n conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>  reshape_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n batch_normalization  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>  conv2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio</span>                                                   \n\n max_pooling2d        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalizat \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                                                        \n\n conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        <span style=\"color: #00af00; text-decoration-color: #00af00\">6,208</span>  max_pooling2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n\n batch_normalizatio  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  conv2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio</span>                                                   \n\n max_pooling2d_1      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalizat \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                                                        \n\n conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span>  max_pooling2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n batch_normalizatio  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  conv2d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio</span>                                                   \n\n max_pooling2d_2      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalizat \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                                                        \n\n flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  max_pooling2d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">66,048</span>  flatten[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n\n reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n\n dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n bidirectional        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,792</span>  reshape[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)                                                       \n\n dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">65,664</span>  dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n\n bidirectional_1      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">98,816</span>  bidirectional[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)                                                       \n\n dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n\n concatenate          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  bidirectional_1[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                       dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span>  concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n\n dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span>  dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m313,354\u001b[0m (1.20 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">313,354</span> (1.20 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m312,906\u001b[0m (1.19 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">312,906</span> (1.19 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m448\u001b[0m (1.75 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> (1.75 KB)\n</pre>\n"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"import numpy as np\nimport librosa\nimport pickle\n\n# Function to extract MFCC features using librosa\ndef extract_mfcc(file_path, n_mfcc=13):\n    try:\n        audio, sample_rate = librosa.load(file_path, sr=None)  # Load audio with its original sample rate\n        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=n_mfcc)\n        return np.mean(mfccs.T, axis=0)  # Return the mean of the MFCCs over time\n    except Exception as e:\n        print(f\"Error extracting MFCC for {file_path}: {e}\")\n        return None\n\n# Lists to store features and labels\nX_balanced = []\ny_balanced = []\n\n# Extract MFCC features from each file in the balanced dataset\nfor index, row in balanced_data.iterrows():\n    file_path = row['path']\n    mfcc_features = extract_mfcc(file_path)\n    if mfcc_features is not None:  # Only add if extraction was successful\n        X_balanced.append(mfcc_features)\n        y_balanced.append(row['emotion_label'])\n\n# Convert lists to arrays (split them here)\nX_balanced = np.array(X_balanced)\ny_balanced = np.array(y_balanced)\n\n# Now you have X_balanced and y_balanced as separate variables\nprint(\"MFCC features and labels are now separated.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T19:12:49.978484Z","iopub.execute_input":"2024-12-09T19:12:49.979271Z","iopub.status.idle":"2024-12-09T19:15:05.319342Z","shell.execute_reply.started":"2024-12-09T19:12:49.979233Z","shell.execute_reply":"2024-12-09T19:15:05.317190Z"}},"outputs":[{"name":"stdout","text":"MFCC features and labels are now separated.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Print the shapes of the feature and label arrays\nprint(f\"Shape of X_balanced (features): {X_balanced.shape}\")\nprint(f\"Shape of y_balanced (labels): {y_balanced.shape}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T19:34:32.376815Z","iopub.execute_input":"2024-12-09T19:34:32.378178Z","iopub.status.idle":"2024-12-09T19:34:32.383780Z","shell.execute_reply.started":"2024-12-09T19:34:32.378117Z","shell.execute_reply":"2024-12-09T19:34:32.382555Z"}},"outputs":[{"name":"stdout","text":"Shape of X_balanced (features): (7460, 13)\nShape of y_balanced (labels): (7460,)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import numpy as np\nimport librosa\nimport pickle\n\n# Function to extract MFCC features using librosa\ndef extract_mfcc(file_path, n_mfcc=13):\n    try:\n        audio, sample_rate = librosa.load(file_path, sr=None)  # Load audio with its original sample rate\n        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=n_mfcc)\n        return mfccs  # Return the MFCCs as a 2D array (n_mfcc x time_frames)\n    except Exception as e:\n        print(f\"Error extracting MFCC for {file_path}: {e}\")\n        return None\n\n# Function to augment audio with time stretch and pitch shift\ndef augment_audio(audio, sample_rate):\n    # Time Stretching\n    stretch_rate = np.random.uniform(0.8, 1.2)  # Stretch factor between 0.8 and 1.2\n    audio_stretched = librosa.effects.time_stretch(audio.astype(float), rate=stretch_rate)\n    \n    # Pitch Shifting\n    n_steps = np.random.randint(-2, 3)  # Random pitch shift between -2 and +2 semitones\n    audio_pitched = librosa.effects.pitch_shift(audio_stretched, sr=sample_rate, n_steps=n_steps)\n    \n    return audio_pitched\n\n# Lists to store features and labels for augmented data\nX_augmented = []\ny_augmented = []\n\n# Apply augmentation (time stretch and pitch shift) to each sample in the balanced dataset\nfor index, row in balanced_data.iterrows():\n    file_path = row['path']\n    mfcc_features = extract_mfcc(file_path)\n    \n    if mfcc_features is not None:\n        # Convert the MFCCs back to audio\n        audio, sample_rate = librosa.load(file_path, sr=None)\n        \n        # Apply augmentation\n        augmented_audio = augment_audio(audio, sample_rate)\n        \n        # Re-extract MFCC from augmented audio\n        augmented_mfcc = librosa.feature.mfcc(y=augmented_audio, sr=sample_rate, n_mfcc=13)\n        \n        # Store the augmented data (mean over time for simplicity)\n        X_augmented.append(np.mean(augmented_mfcc.T, axis=0))\n        y_augmented.append(row['emotion_label'])\n\n# Convert lists to arrays\nX_augmented = np.array(X_augmented)\ny_augmented = np.array(y_augmented)\n\n# Print the shape of the augmented data\nprint(f\"Shape of X_augmented (features): {X_augmented.shape}\")\nprint(f\"Shape of y_augmented (labels): {y_augmented.shape}\")\n\n# Save the augmented features and labels as a pickle file\nwith open('augmented_mfcc_features.pkl', 'wb') as f:\n    pickle.dump((X_augmented, y_augmented), f)\n\nprint(\"Augmented MFCC features and labels saved to 'augmented_mfcc_features.pkl'.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T20:29:37.342845Z","iopub.execute_input":"2024-11-13T20:29:37.343230Z","iopub.status.idle":"2024-11-13T20:44:57.833331Z","shell.execute_reply.started":"2024-11-13T20:29:37.343192Z","shell.execute_reply":"2024-11-13T20:44:57.832297Z"}},"outputs":[{"name":"stdout","text":"Shape of X_augmented (features): (7460, 13)\nShape of y_augmented (labels): (7460,)\nAugmented MFCC features and labels saved to 'augmented_mfcc_features.pkl'.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import LabelBinarizer\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, LSTM, Bidirectional, Conv2D, BatchNormalization, MaxPooling2D, Flatten, Dense, Dropout, Concatenate, Reshape\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n\n# Assuming X_augmented and y_augmented are already defined and reshaped\nX = X_augmented  # Features\ny = y_augmented  # Labels\n\n# One-hot encode the labels\nlb = LabelBinarizer()\ny_categorical = lb.fit_transform(y)\n\n# Define the model function\ndef create_model(input_shape, num_classes):\n    input_layer = Input(shape=input_shape)\n    \n    # BiLSTM branch\n    bilstm_input = Reshape((13, 1))(input_layer)\n    bilstm1 = Bidirectional(LSTM(64, return_sequences=True))(bilstm_input)\n    bilstm2 = Bidirectional(LSTM(64))(bilstm1)\n\n    # Conv2D branch\n    conv_input = Reshape((13, 1, 1))(input_layer)\n    conv2d = Conv2D(32, kernel_size=(3, 1), dilation_rate=(2, 1), activation='relu', padding='same')(conv_input)\n    conv2d = BatchNormalization()(conv2d)\n    conv2d = MaxPooling2D(pool_size=(2, 1))(conv2d)\n\n    conv2d = Conv2D(64, kernel_size=(3, 1), dilation_rate=(2, 1), activation='relu', padding='same')(conv2d)\n    conv2d = BatchNormalization()(conv2d)\n    conv2d = MaxPooling2D(pool_size=(2, 1))(conv2d)\n\n    conv2d = Conv2D(128, kernel_size=(3, 1), dilation_rate=(2, 1), activation='relu', padding='same')(conv2d)\n    conv2d = BatchNormalization()(conv2d)\n    conv2d = MaxPooling2D(pool_size=(2, 1))(conv2d)\n\n    # Flatten and dense layers for Conv2D branch\n    conv_flat = Flatten()(conv2d)\n    dense1 = Dense(512, activation='relu')(conv_flat)\n    dropout4 = Dropout(0.2)(dense1)\n    dense2 = Dense(128, activation='relu')(dropout4)\n    dropout5 = Dropout(0.2)(dense2)\n\n    # Concatenate BiLSTM and Conv2D branches\n    concatenated = Concatenate()([bilstm2, dropout5])\n    dense = Dense(64, activation='relu')(concatenated)\n    output_layer = Dense(num_classes, activation='softmax')(dense)\n\n    # Model definition and compilation\n    model = Model(inputs=input_layer, outputs=output_layer)\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    \n    return model\n\n# K-Fold Cross-Validation\nkf = KFold(n_splits=5, shuffle=True, random_state=42)  # 5-fold cross-validation\nfold_no = 1\nfold_accuracies = []\n\nfor train_index, val_index in kf.split(X):\n    print(f\"\\nTraining fold {fold_no}...\")\n\n    # Split data into train and validation for this fold\n    X_train, X_val = X[train_index], X[val_index]\n    y_train, y_val = y_categorical[train_index], y_categorical[val_index]\n    \n    # Reshape the data as needed\n    X_train = X_train.reshape((-1, 13, 1))\n    X_val = X_val.reshape((-1, 13, 1))\n\n    # Create the model\n    model = create_model((13, 1), y_train.shape[1])\n\n    # Callbacks\n    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6)\n    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n\n    # Train the model\n    history = model.fit(\n        X_train, y_train,\n        validation_data=(X_val, y_val),\n        batch_size=32,\n        epochs=30,\n        callbacks=[reduce_lr, early_stopping],\n        verbose=1\n    )\n\n    # Evaluate the model\n    val_loss, val_accuracy = model.evaluate(X_val, y_val, verbose=0)\n    print(f\"Validation accuracy for fold {fold_no}: {val_accuracy}\")\n    fold_accuracies.append(val_accuracy)\n    \n    fold_no += 1\n\n# Calculate the average accuracy across all folds\naverage_accuracy = np.mean(fold_accuracies)\nprint(f\"\\nAverage Cross-Validation Accuracy: {average_accuracy}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T20:45:46.981248Z","iopub.execute_input":"2024-11-13T20:45:46.981642Z","iopub.status.idle":"2024-11-13T20:53:48.892297Z","shell.execute_reply.started":"2024-11-13T20:45:46.981605Z","shell.execute_reply":"2024-11-13T20:53:48.891365Z"}},"outputs":[{"name":"stdout","text":"\nTraining fold 1...\nEpoch 1/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 19ms/step - accuracy: 0.3372 - loss: 1.8217 - val_accuracy: 0.3264 - val_loss: 1.8430 - learning_rate: 0.0010\nEpoch 2/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.4798 - loss: 1.4090 - val_accuracy: 0.4638 - val_loss: 1.4276 - learning_rate: 0.0010\nEpoch 3/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.5126 - loss: 1.3405 - val_accuracy: 0.5422 - val_loss: 1.2624 - learning_rate: 0.0010\nEpoch 4/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.5648 - loss: 1.2201 - val_accuracy: 0.5127 - val_loss: 1.3443 - learning_rate: 0.0010\nEpoch 5/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.5768 - loss: 1.1698 - val_accuracy: 0.5536 - val_loss: 1.2168 - learning_rate: 0.0010\nEpoch 6/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.5783 - loss: 1.1482 - val_accuracy: 0.5791 - val_loss: 1.1723 - learning_rate: 0.0010\nEpoch 7/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.6114 - loss: 1.0647 - val_accuracy: 0.5275 - val_loss: 1.2837 - learning_rate: 0.0010\nEpoch 8/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.6309 - loss: 1.0221 - val_accuracy: 0.5938 - val_loss: 1.0967 - learning_rate: 0.0010\nEpoch 9/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.6373 - loss: 0.9819 - val_accuracy: 0.6166 - val_loss: 1.0375 - learning_rate: 0.0010\nEpoch 10/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.6447 - loss: 0.9458 - val_accuracy: 0.6126 - val_loss: 1.0595 - learning_rate: 0.0010\nEpoch 11/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.6637 - loss: 0.8947 - val_accuracy: 0.6126 - val_loss: 1.0656 - learning_rate: 0.0010\nEpoch 12/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.6678 - loss: 0.8774 - val_accuracy: 0.6160 - val_loss: 1.0917 - learning_rate: 0.0010\nEpoch 13/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.6894 - loss: 0.8235 - val_accuracy: 0.6119 - val_loss: 1.0847 - learning_rate: 0.0010\nEpoch 14/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.7117 - loss: 0.7791 - val_accuracy: 0.6414 - val_loss: 0.9619 - learning_rate: 0.0010\nEpoch 15/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.7130 - loss: 0.7830 - val_accuracy: 0.6294 - val_loss: 1.0512 - learning_rate: 0.0010\nEpoch 16/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.7268 - loss: 0.7296 - val_accuracy: 0.6468 - val_loss: 0.9664 - learning_rate: 0.0010\nEpoch 17/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.7281 - loss: 0.7243 - val_accuracy: 0.6461 - val_loss: 0.9595 - learning_rate: 0.0010\nEpoch 18/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.7483 - loss: 0.6686 - val_accuracy: 0.6635 - val_loss: 0.9126 - learning_rate: 0.0010\nEpoch 19/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.7608 - loss: 0.6682 - val_accuracy: 0.6743 - val_loss: 0.9138 - learning_rate: 0.0010\nEpoch 20/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.7667 - loss: 0.6287 - val_accuracy: 0.6521 - val_loss: 0.9485 - learning_rate: 0.0010\nEpoch 21/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.7977 - loss: 0.5704 - val_accuracy: 0.6836 - val_loss: 0.9205 - learning_rate: 0.0010\nEpoch 22/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.7907 - loss: 0.5568 - val_accuracy: 0.6655 - val_loss: 0.9722 - learning_rate: 0.0010\nEpoch 23/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.8059 - loss: 0.5245 - val_accuracy: 0.6669 - val_loss: 0.9624 - learning_rate: 0.0010\nEpoch 24/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.8397 - loss: 0.4383 - val_accuracy: 0.7192 - val_loss: 0.8455 - learning_rate: 2.0000e-04\nEpoch 25/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.8689 - loss: 0.3770 - val_accuracy: 0.7151 - val_loss: 0.8788 - learning_rate: 2.0000e-04\nEpoch 26/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.8834 - loss: 0.3472 - val_accuracy: 0.7051 - val_loss: 0.8782 - learning_rate: 2.0000e-04\nEpoch 27/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.8738 - loss: 0.3409 - val_accuracy: 0.7165 - val_loss: 0.9020 - learning_rate: 2.0000e-04\nEpoch 28/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.8928 - loss: 0.3136 - val_accuracy: 0.7151 - val_loss: 0.8783 - learning_rate: 2.0000e-04\nEpoch 29/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.8895 - loss: 0.3153 - val_accuracy: 0.7178 - val_loss: 0.8978 - learning_rate: 2.0000e-04\nEpoch 30/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.9179 - loss: 0.2678 - val_accuracy: 0.7151 - val_loss: 0.8884 - learning_rate: 4.0000e-05\nValidation accuracy for fold 1: 0.7191689014434814\n\nTraining fold 2...\nEpoch 1/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - accuracy: 0.3115 - loss: 1.9004 - val_accuracy: 0.2969 - val_loss: 1.8762 - learning_rate: 0.0010\nEpoch 2/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.4740 - loss: 1.4551 - val_accuracy: 0.4216 - val_loss: 1.5797 - learning_rate: 0.0010\nEpoch 3/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.5009 - loss: 1.3759 - val_accuracy: 0.4712 - val_loss: 1.4511 - learning_rate: 0.0010\nEpoch 4/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.5172 - loss: 1.3013 - val_accuracy: 0.5630 - val_loss: 1.2115 - learning_rate: 0.0010\nEpoch 5/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.5560 - loss: 1.2034 - val_accuracy: 0.4886 - val_loss: 1.3750 - learning_rate: 0.0010\nEpoch 6/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.5770 - loss: 1.1557 - val_accuracy: 0.5241 - val_loss: 1.2819 - learning_rate: 0.0010\nEpoch 7/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.5969 - loss: 1.1110 - val_accuracy: 0.5328 - val_loss: 1.2868 - learning_rate: 0.0010\nEpoch 8/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.6134 - loss: 1.0623 - val_accuracy: 0.5617 - val_loss: 1.1347 - learning_rate: 0.0010\nEpoch 9/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.6334 - loss: 0.9950 - val_accuracy: 0.6092 - val_loss: 1.0400 - learning_rate: 0.0010\nEpoch 10/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.6445 - loss: 0.9539 - val_accuracy: 0.5838 - val_loss: 1.1673 - learning_rate: 0.0010\nEpoch 11/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.6529 - loss: 0.9360 - val_accuracy: 0.6166 - val_loss: 1.0308 - learning_rate: 0.0010\nEpoch 12/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.6660 - loss: 0.8767 - val_accuracy: 0.6273 - val_loss: 1.0186 - learning_rate: 0.0010\nEpoch 13/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.6834 - loss: 0.8489 - val_accuracy: 0.6086 - val_loss: 1.0599 - learning_rate: 0.0010\nEpoch 14/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.6943 - loss: 0.8235 - val_accuracy: 0.6220 - val_loss: 0.9742 - learning_rate: 0.0010\nEpoch 15/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.7182 - loss: 0.7724 - val_accuracy: 0.6394 - val_loss: 0.9401 - learning_rate: 0.0010\nEpoch 16/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.7266 - loss: 0.7348 - val_accuracy: 0.6441 - val_loss: 0.9618 - learning_rate: 0.0010\nEpoch 17/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.7404 - loss: 0.7070 - val_accuracy: 0.6347 - val_loss: 0.9943 - learning_rate: 0.0010\nEpoch 18/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.7599 - loss: 0.6458 - val_accuracy: 0.6414 - val_loss: 0.9988 - learning_rate: 0.0010\nEpoch 19/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.7560 - loss: 0.6493 - val_accuracy: 0.6776 - val_loss: 0.8881 - learning_rate: 0.0010\nEpoch 20/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.7778 - loss: 0.6014 - val_accuracy: 0.6595 - val_loss: 0.9462 - learning_rate: 0.0010\nEpoch 21/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.7779 - loss: 0.5971 - val_accuracy: 0.6421 - val_loss: 0.9912 - learning_rate: 0.0010\nEpoch 22/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.8020 - loss: 0.5310 - val_accuracy: 0.6723 - val_loss: 0.9219 - learning_rate: 0.0010\nEpoch 23/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.8027 - loss: 0.5267 - val_accuracy: 0.6803 - val_loss: 0.9255 - learning_rate: 0.0010\nEpoch 24/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.8210 - loss: 0.4773 - val_accuracy: 0.6736 - val_loss: 0.9179 - learning_rate: 0.0010\nEpoch 25/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.8549 - loss: 0.4129 - val_accuracy: 0.7031 - val_loss: 0.8316 - learning_rate: 2.0000e-04\nEpoch 26/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.8898 - loss: 0.3342 - val_accuracy: 0.7105 - val_loss: 0.8430 - learning_rate: 2.0000e-04\nEpoch 27/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.9017 - loss: 0.2956 - val_accuracy: 0.7017 - val_loss: 0.8490 - learning_rate: 2.0000e-04\nEpoch 28/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.9102 - loss: 0.2736 - val_accuracy: 0.7058 - val_loss: 0.8559 - learning_rate: 2.0000e-04\nEpoch 29/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.9055 - loss: 0.2772 - val_accuracy: 0.7125 - val_loss: 0.8635 - learning_rate: 2.0000e-04\nEpoch 30/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.9190 - loss: 0.2600 - val_accuracy: 0.7192 - val_loss: 0.8724 - learning_rate: 2.0000e-04\nValidation accuracy for fold 2: 0.7030830979347229\n\nTraining fold 3...\nEpoch 1/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - accuracy: 0.3325 - loss: 1.8106 - val_accuracy: 0.2748 - val_loss: 1.9500 - learning_rate: 0.0010\nEpoch 2/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.4764 - loss: 1.4253 - val_accuracy: 0.3847 - val_loss: 1.6491 - learning_rate: 0.0010\nEpoch 3/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.5078 - loss: 1.3434 - val_accuracy: 0.5248 - val_loss: 1.2802 - learning_rate: 0.0010\nEpoch 4/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.5489 - loss: 1.2414 - val_accuracy: 0.5020 - val_loss: 1.3449 - learning_rate: 0.0010\nEpoch 5/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.5649 - loss: 1.1928 - val_accuracy: 0.5456 - val_loss: 1.2007 - learning_rate: 0.0010\nEpoch 6/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.5890 - loss: 1.0966 - val_accuracy: 0.6079 - val_loss: 1.0707 - learning_rate: 0.0010\nEpoch 7/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.6010 - loss: 1.0781 - val_accuracy: 0.5784 - val_loss: 1.1251 - learning_rate: 0.0010\nEpoch 8/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.6156 - loss: 1.0376 - val_accuracy: 0.6267 - val_loss: 1.0126 - learning_rate: 0.0010\nEpoch 9/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.6455 - loss: 0.9433 - val_accuracy: 0.6166 - val_loss: 1.0707 - learning_rate: 0.0010\nEpoch 10/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.6566 - loss: 0.9285 - val_accuracy: 0.6066 - val_loss: 1.0808 - learning_rate: 0.0010\nEpoch 11/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.6690 - loss: 0.8681 - val_accuracy: 0.6387 - val_loss: 0.9653 - learning_rate: 0.0010\nEpoch 12/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.6948 - loss: 0.8150 - val_accuracy: 0.6287 - val_loss: 1.0002 - learning_rate: 0.0010\nEpoch 13/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.7050 - loss: 0.7835 - val_accuracy: 0.6421 - val_loss: 0.9485 - learning_rate: 0.0010\nEpoch 14/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.7109 - loss: 0.7818 - val_accuracy: 0.6280 - val_loss: 1.0361 - learning_rate: 0.0010\nEpoch 15/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.7242 - loss: 0.7459 - val_accuracy: 0.6488 - val_loss: 0.9854 - learning_rate: 0.0010\nEpoch 16/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.7348 - loss: 0.7255 - val_accuracy: 0.6655 - val_loss: 0.9275 - learning_rate: 0.0010\nEpoch 17/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.7535 - loss: 0.6560 - val_accuracy: 0.6501 - val_loss: 0.9550 - learning_rate: 0.0010\nEpoch 18/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.7640 - loss: 0.6498 - val_accuracy: 0.6508 - val_loss: 0.9822 - learning_rate: 0.0010\nEpoch 19/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.7697 - loss: 0.6244 - val_accuracy: 0.6843 - val_loss: 0.8842 - learning_rate: 0.0010\nEpoch 20/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.7878 - loss: 0.5844 - val_accuracy: 0.6635 - val_loss: 0.9577 - learning_rate: 0.0010\nEpoch 21/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.7955 - loss: 0.5528 - val_accuracy: 0.6635 - val_loss: 1.0057 - learning_rate: 0.0010\nEpoch 22/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.7939 - loss: 0.5473 - val_accuracy: 0.6796 - val_loss: 0.9052 - learning_rate: 0.0010\nEpoch 23/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.8263 - loss: 0.4892 - val_accuracy: 0.6588 - val_loss: 0.9774 - learning_rate: 0.0010\nEpoch 24/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.8291 - loss: 0.4626 - val_accuracy: 0.6602 - val_loss: 1.0204 - learning_rate: 0.0010\nEpoch 25/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.8516 - loss: 0.3941 - val_accuracy: 0.7158 - val_loss: 0.8466 - learning_rate: 2.0000e-04\nEpoch 26/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.8940 - loss: 0.3065 - val_accuracy: 0.7218 - val_loss: 0.8492 - learning_rate: 2.0000e-04\nEpoch 27/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.9071 - loss: 0.2840 - val_accuracy: 0.7286 - val_loss: 0.8699 - learning_rate: 2.0000e-04\nEpoch 28/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.9093 - loss: 0.2690 - val_accuracy: 0.7286 - val_loss: 0.8729 - learning_rate: 2.0000e-04\nEpoch 29/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.9195 - loss: 0.2411 - val_accuracy: 0.7259 - val_loss: 0.8712 - learning_rate: 2.0000e-04\nEpoch 30/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.9152 - loss: 0.2539 - val_accuracy: 0.7339 - val_loss: 0.8843 - learning_rate: 2.0000e-04\nValidation accuracy for fold 3: 0.7158176898956299\n\nTraining fold 4...\nEpoch 1/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - accuracy: 0.3360 - loss: 1.8383 - val_accuracy: 0.2286 - val_loss: 1.9308 - learning_rate: 0.0010\nEpoch 2/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.4761 - loss: 1.4643 - val_accuracy: 0.4980 - val_loss: 1.4263 - learning_rate: 0.0010\nEpoch 3/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.4966 - loss: 1.3427 - val_accuracy: 0.5228 - val_loss: 1.2777 - learning_rate: 0.0010\nEpoch 4/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.5467 - loss: 1.2282 - val_accuracy: 0.5630 - val_loss: 1.2157 - learning_rate: 0.0010\nEpoch 5/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.5779 - loss: 1.1521 - val_accuracy: 0.5637 - val_loss: 1.1663 - learning_rate: 0.0010\nEpoch 6/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.5864 - loss: 1.1143 - val_accuracy: 0.5436 - val_loss: 1.2350 - learning_rate: 0.0010\nEpoch 7/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.6233 - loss: 1.0132 - val_accuracy: 0.6099 - val_loss: 1.0530 - learning_rate: 0.0010\nEpoch 8/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.6335 - loss: 0.9958 - val_accuracy: 0.6200 - val_loss: 1.0204 - learning_rate: 0.0010\nEpoch 9/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.6539 - loss: 0.9432 - val_accuracy: 0.6334 - val_loss: 1.0054 - learning_rate: 0.0010\nEpoch 10/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.6655 - loss: 0.8817 - val_accuracy: 0.6300 - val_loss: 1.0045 - learning_rate: 0.0010\nEpoch 11/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.6829 - loss: 0.8554 - val_accuracy: 0.6448 - val_loss: 0.9703 - learning_rate: 0.0010\nEpoch 12/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.6938 - loss: 0.8224 - val_accuracy: 0.6468 - val_loss: 0.9642 - learning_rate: 0.0010\nEpoch 13/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.7052 - loss: 0.8029 - val_accuracy: 0.6488 - val_loss: 0.9540 - learning_rate: 0.0010\nEpoch 14/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.7103 - loss: 0.7594 - val_accuracy: 0.6669 - val_loss: 0.9525 - learning_rate: 0.0010\nEpoch 15/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.7221 - loss: 0.7387 - val_accuracy: 0.6562 - val_loss: 0.9528 - learning_rate: 0.0010\nEpoch 16/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.7367 - loss: 0.7040 - val_accuracy: 0.6689 - val_loss: 0.9006 - learning_rate: 0.0010\nEpoch 17/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.7528 - loss: 0.6630 - val_accuracy: 0.6696 - val_loss: 0.9252 - learning_rate: 0.0010\nEpoch 18/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.7575 - loss: 0.6535 - val_accuracy: 0.6723 - val_loss: 0.9338 - learning_rate: 0.0010\nEpoch 19/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.7789 - loss: 0.6005 - val_accuracy: 0.6783 - val_loss: 0.9175 - learning_rate: 0.0010\nEpoch 20/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.7981 - loss: 0.5527 - val_accuracy: 0.6696 - val_loss: 0.9693 - learning_rate: 0.0010\nEpoch 21/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.7913 - loss: 0.5436 - val_accuracy: 0.7044 - val_loss: 0.9120 - learning_rate: 0.0010\nEpoch 22/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.8167 - loss: 0.4944 - val_accuracy: 0.7252 - val_loss: 0.8314 - learning_rate: 2.0000e-04\nEpoch 23/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.8633 - loss: 0.3789 - val_accuracy: 0.7245 - val_loss: 0.8445 - learning_rate: 2.0000e-04\nEpoch 24/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.8782 - loss: 0.3505 - val_accuracy: 0.7272 - val_loss: 0.8555 - learning_rate: 2.0000e-04\nEpoch 25/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.8915 - loss: 0.3236 - val_accuracy: 0.7306 - val_loss: 0.8805 - learning_rate: 2.0000e-04\nEpoch 26/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.8900 - loss: 0.3164 - val_accuracy: 0.7312 - val_loss: 0.8697 - learning_rate: 2.0000e-04\nEpoch 27/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.8862 - loss: 0.3230 - val_accuracy: 0.7265 - val_loss: 0.8693 - learning_rate: 2.0000e-04\nEpoch 28/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.8960 - loss: 0.3044 - val_accuracy: 0.7413 - val_loss: 0.8676 - learning_rate: 4.0000e-05\nEpoch 29/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.9075 - loss: 0.2835 - val_accuracy: 0.7406 - val_loss: 0.8673 - learning_rate: 4.0000e-05\nEpoch 30/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.9032 - loss: 0.2867 - val_accuracy: 0.7373 - val_loss: 0.8743 - learning_rate: 4.0000e-05\nValidation accuracy for fold 4: 0.7252010703086853\n\nTraining fold 5...\nEpoch 1/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - accuracy: 0.3109 - loss: 1.8875 - val_accuracy: 0.1836 - val_loss: 2.0682 - learning_rate: 0.0010\nEpoch 2/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.4676 - loss: 1.4751 - val_accuracy: 0.4471 - val_loss: 1.5705 - learning_rate: 0.0010\nEpoch 3/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.4758 - loss: 1.3851 - val_accuracy: 0.4759 - val_loss: 1.4417 - learning_rate: 0.0010\nEpoch 4/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.5257 - loss: 1.2929 - val_accuracy: 0.5375 - val_loss: 1.2405 - learning_rate: 0.0010\nEpoch 5/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.5727 - loss: 1.1931 - val_accuracy: 0.5744 - val_loss: 1.1873 - learning_rate: 0.0010\nEpoch 6/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.5665 - loss: 1.1713 - val_accuracy: 0.5865 - val_loss: 1.1475 - learning_rate: 0.0010\nEpoch 7/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.5885 - loss: 1.1107 - val_accuracy: 0.5617 - val_loss: 1.1896 - learning_rate: 0.0010\nEpoch 8/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.5985 - loss: 1.0650 - val_accuracy: 0.6173 - val_loss: 1.0750 - learning_rate: 0.0010\nEpoch 9/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.6174 - loss: 0.9985 - val_accuracy: 0.5851 - val_loss: 1.1170 - learning_rate: 0.0010\nEpoch 10/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.6328 - loss: 0.9880 - val_accuracy: 0.6488 - val_loss: 0.9694 - learning_rate: 0.0010\nEpoch 11/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.6571 - loss: 0.9169 - val_accuracy: 0.6635 - val_loss: 0.9452 - learning_rate: 0.0010\nEpoch 12/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.6804 - loss: 0.8753 - val_accuracy: 0.6287 - val_loss: 1.0315 - learning_rate: 0.0010\nEpoch 13/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.6967 - loss: 0.8198 - val_accuracy: 0.6521 - val_loss: 0.9802 - learning_rate: 0.0010\nEpoch 14/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.7067 - loss: 0.7915 - val_accuracy: 0.6702 - val_loss: 0.9135 - learning_rate: 0.0010\nEpoch 15/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.7288 - loss: 0.7458 - val_accuracy: 0.6783 - val_loss: 0.9206 - learning_rate: 0.0010\nEpoch 16/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.7323 - loss: 0.7341 - val_accuracy: 0.6870 - val_loss: 0.8596 - learning_rate: 0.0010\nEpoch 17/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.7274 - loss: 0.7229 - val_accuracy: 0.6723 - val_loss: 0.9030 - learning_rate: 0.0010\nEpoch 18/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.7596 - loss: 0.6474 - val_accuracy: 0.6857 - val_loss: 0.9128 - learning_rate: 0.0010\nEpoch 19/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.7598 - loss: 0.6513 - val_accuracy: 0.7031 - val_loss: 0.8502 - learning_rate: 0.0010\nEpoch 20/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.7738 - loss: 0.5985 - val_accuracy: 0.7105 - val_loss: 0.8327 - learning_rate: 0.0010\nEpoch 21/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.7891 - loss: 0.5688 - val_accuracy: 0.6971 - val_loss: 0.8755 - learning_rate: 0.0010\nEpoch 22/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.8082 - loss: 0.5305 - val_accuracy: 0.6991 - val_loss: 0.8657 - learning_rate: 0.0010\nEpoch 23/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.7986 - loss: 0.5424 - val_accuracy: 0.6903 - val_loss: 0.9016 - learning_rate: 0.0010\nEpoch 24/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.8252 - loss: 0.4731 - val_accuracy: 0.7138 - val_loss: 0.9090 - learning_rate: 0.0010\nEpoch 25/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.8325 - loss: 0.4679 - val_accuracy: 0.6971 - val_loss: 0.9628 - learning_rate: 0.0010\nEpoch 26/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.8657 - loss: 0.3844 - val_accuracy: 0.7487 - val_loss: 0.7979 - learning_rate: 2.0000e-04\nEpoch 27/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.8932 - loss: 0.3123 - val_accuracy: 0.7527 - val_loss: 0.7919 - learning_rate: 2.0000e-04\nEpoch 28/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.9064 - loss: 0.2871 - val_accuracy: 0.7554 - val_loss: 0.8075 - learning_rate: 2.0000e-04\nEpoch 29/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.9125 - loss: 0.2630 - val_accuracy: 0.7527 - val_loss: 0.8118 - learning_rate: 2.0000e-04\nEpoch 30/30\n\u001b[1m187/187\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.9110 - loss: 0.2606 - val_accuracy: 0.7520 - val_loss: 0.8085 - learning_rate: 2.0000e-04\nValidation accuracy for fold 5: 0.7526809573173523\n\nAverage Cross-Validation Accuracy: 0.7231903433799743\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# Emo-Net","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport librosa\nimport pickle\n\n# Function to extract MFCC features using librosa\ndef extract_mfcc(file_path, n_mfcc=13):\n    try:\n        audio, sample_rate = librosa.load(file_path, sr=None)  # Load audio with its original sample rate\n        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=n_mfcc)\n        return mfccs  # Return the MFCCs as a 2D array (n_mfcc x time_frames)\n    except Exception as e:\n        print(f\"Error extracting MFCC for {file_path}: {e}\")\n        return None\n\n# Function to augment audio with time stretch and pitch shift\ndef augment_audio(audio, sample_rate):\n    augmented_samples = []\n    \n    # Apply multiple augmentations\n    for _ in range(3):  # Apply 3 augmentations per sample\n        # Time Stretching\n        stretch_rate = np.random.uniform(0.8, 1.2)  # Stretch factor between 0.8 and 1.2\n        audio_stretched = librosa.effects.time_stretch(audio.astype(float), rate=stretch_rate)\n        \n        # Pitch Shifting\n        n_steps = np.random.randint(-2, 3)  # Random pitch shift between -2 and +2 semitones\n        audio_pitched = librosa.effects.pitch_shift(audio_stretched, sr=sample_rate, n_steps=n_steps)\n        \n        augmented_samples.append(audio_pitched)\n    \n    return augmented_samples\n\n# Lists to store features and labels for augmented data\nX_augmented = []\ny_augmented = []\n\n# Apply augmentation (time stretch and pitch shift) to each sample in the balanced dataset\nfor index, row in balanced_data.iterrows():\n    file_path = row['path']\n    mfcc_features = extract_mfcc(file_path)\n    \n    if mfcc_features is not None:\n        # Convert the MFCCs back to audio\n        audio, sample_rate = librosa.load(file_path, sr=None)\n        \n        # Apply augmentation (get 3 augmented samples per original audio)\n        augmented_audios = augment_audio(audio, sample_rate)\n        \n        # For each augmented audio, extract MFCC and append\n        for augmented_audio in augmented_audios:\n            augmented_mfcc = librosa.feature.mfcc(y=augmented_audio, sr=sample_rate, n_mfcc=13)\n            X_augmented.append(np.mean(augmented_mfcc.T, axis=0))  # Take mean over time for simplicity\n            y_augmented.append(row['emotion_label'])\n\n# Convert lists to arrays\nX_augmented = np.array(X_augmented)\ny_augmented = np.array(y_augmented)\n\n# Print the shape of the augmented data\nprint(f\"Shape of X_augmented (features): {X_augmented.shape}\")\nprint(f\"Shape of y_augmented (labels): {y_augmented.shape}\")\n\n# Save the augmented features and labels as a pickle file\nwith open('augmented_mfcc_features.pkl', 'wb') as f:\n    pickle.dump((X_augmented, y_augmented), f)\n\nprint(\"Augmented MFCC features and labels saved to 'augmented_mfcc_features.pkl'.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T19:34:59.400779Z","iopub.execute_input":"2024-12-09T19:34:59.401538Z","iopub.status.idle":"2024-12-09T20:05:58.412852Z","shell.execute_reply.started":"2024-12-09T19:34:59.401492Z","shell.execute_reply":"2024-12-09T20:05:58.409807Z"}},"outputs":[{"name":"stdout","text":"Shape of X_augmented (features): (22380, 13)\nShape of y_augmented (labels): (22380,)\nAugmented MFCC features and labels saved to 'augmented_mfcc_features.pkl'.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import LabelBinarizer\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Bidirectional, LSTM, Dense, Reshape, Conv2D, BatchNormalization, MaxPooling2D, Flatten, Dropout, Concatenate\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom tensorflow.keras.utils import to_categorical\n\n# Assuming X_augmented and y_augmented are the augmented data features and labels\n\n# One-hot encode the labels\nlb = LabelBinarizer()\ny_augmented_categorical = lb.fit_transform(y_augmented)\n\n# Reshape the features as needed (13 MFCC coefficients per time frame)\nX_augmented_reshaped = X_augmented.reshape((-1, 13, 1))\n\n# Initialize KFold\nkfold = KFold(n_splits=5, shuffle=True, random_state=42)\nfold_no = 1\nval_accuracies = []\n\n# K-Fold Cross Validation\nfor train_index, val_index in kfold.split(X_augmented_reshaped, y_augmented_categorical):\n    print(f\"Training fold {fold_no}...\")\n    \n    # Split data into training and validation sets\n    X_train, X_val = X_augmented_reshaped[train_index], X_augmented_reshaped[val_index]\n    y_train, y_val = y_augmented_categorical[train_index], y_augmented_categorical[val_index]\n    \n    # Define the model (BiLSTM + Conv2D with dilated convolutions)\n    input_shape = (13, 1)\n    input_layer = Input(shape=input_shape)\n    \n    # BiLSTM branch\n    bilstm_input_model1 = Reshape((13, 1))(input_layer)\n    bilstm1_model1 = Bidirectional(LSTM(64, return_sequences=True))(bilstm_input_model1)\n    bilstm2_model1 = Bidirectional(LSTM(64))(bilstm1_model1)\n    \n    # Conv2D branch with Dilated Convolutions\n    conv_input = Reshape((13, 1, 1))(input_layer)\n    conv2d = Conv2D(32, kernel_size=(3, 1), dilation_rate=(2, 1), activation='relu', padding='same')(conv_input)\n    conv2d = BatchNormalization()(conv2d)\n    conv2d = MaxPooling2D(pool_size=(2, 1))(conv2d)\n    \n    conv2d = Conv2D(64, kernel_size=(3, 1), dilation_rate=(2, 1), activation='relu', padding='same')(conv2d)\n    conv2d = BatchNormalization()(conv2d)\n    conv2d = MaxPooling2D(pool_size=(2, 1))(conv2d)\n    \n    conv2d = Conv2D(128, kernel_size=(3, 1), dilation_rate=(2, 1), activation='relu', padding='same')(conv2d)\n    conv2d = BatchNormalization()(conv2d)\n    conv2d = MaxPooling2D(pool_size=(2, 1))(conv2d)\n    \n    # Flatten and dense layers for Conv2D branch\n    conv_flat = Flatten()(conv2d)\n    dense1 = Dense(512, activation='relu')(conv_flat)\n    dropout4 = Dropout(0.2)(dense1)\n    dense2 = Dense(128, activation='relu')(dropout4)\n    dropout5 = Dropout(0.2)(dense2)\n    \n    # Concatenate BiLSTM and Conv2D branches\n    concatenated = Concatenate()([bilstm2_model1, dropout5])\n    dense = Dense(64, activation='relu')(concatenated)\n    output_layer = Dense(y_train.shape[1], activation='softmax')(dense)\n    \n    # Model definition and compilation\n    model_speech = Model(inputs=input_layer, outputs=output_layer)\n    model_speech.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    \n    # Callbacks for training\n    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6)\n    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n    \n    # Train the model\n    history = model_speech.fit(\n        X_train, y_train,\n        validation_data=(X_val, y_val),\n        batch_size=32,\n        epochs=30,\n        callbacks=[reduce_lr, early_stopping]\n    )\n    \n    # Get the validation accuracy and store it\n    val_accuracy = history.history['val_accuracy'][-1]\n    val_accuracies.append(val_accuracy)\n    \n    # Print validation accuracy for the fold\n    print(f\"Validation accuracy for fold {fold_no}: {val_accuracy:.4f}\")\n    \n    fold_no += 1\n\n# Print the average validation accuracy across all folds\navg_val_accuracy = np.mean(val_accuracies)\nprint(f\"\\nAverage validation accuracy across all folds: {avg_val_accuracy:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T22:07:57.461708Z","iopub.execute_input":"2024-11-13T22:07:57.462607Z","iopub.status.idle":"2024-11-13T22:30:36.727802Z","shell.execute_reply.started":"2024-11-13T22:07:57.462565Z","shell.execute_reply":"2024-11-13T22:30:36.726776Z"}},"outputs":[{"name":"stdout","text":"Training fold 1...\nEpoch 1/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - accuracy: 0.3630 - loss: 1.7268 - val_accuracy: 0.5337 - val_loss: 1.3021 - learning_rate: 0.0010\nEpoch 2/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.5237 - loss: 1.2841 - val_accuracy: 0.5786 - val_loss: 1.1516 - learning_rate: 0.0010\nEpoch 3/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.5899 - loss: 1.1088 - val_accuracy: 0.6296 - val_loss: 1.0240 - learning_rate: 0.0010\nEpoch 4/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - accuracy: 0.6246 - loss: 1.0090 - val_accuracy: 0.6439 - val_loss: 0.9793 - learning_rate: 0.0010\nEpoch 5/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.6525 - loss: 0.9304 - val_accuracy: 0.6591 - val_loss: 0.9126 - learning_rate: 0.0010\nEpoch 6/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - accuracy: 0.6859 - loss: 0.8373 - val_accuracy: 0.6859 - val_loss: 0.8547 - learning_rate: 0.0010\nEpoch 7/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.7114 - loss: 0.7780 - val_accuracy: 0.7294 - val_loss: 0.7398 - learning_rate: 0.0010\nEpoch 8/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.7228 - loss: 0.7341 - val_accuracy: 0.7346 - val_loss: 0.7152 - learning_rate: 0.0010\nEpoch 9/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.7515 - loss: 0.6771 - val_accuracy: 0.7522 - val_loss: 0.6880 - learning_rate: 0.0010\nEpoch 10/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.7688 - loss: 0.6286 - val_accuracy: 0.7587 - val_loss: 0.6397 - learning_rate: 0.0010\nEpoch 11/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.7836 - loss: 0.5844 - val_accuracy: 0.7808 - val_loss: 0.6103 - learning_rate: 0.0010\nEpoch 12/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.7906 - loss: 0.5602 - val_accuracy: 0.7864 - val_loss: 0.6005 - learning_rate: 0.0010\nEpoch 13/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.8154 - loss: 0.5067 - val_accuracy: 0.7835 - val_loss: 0.6043 - learning_rate: 0.0010\nEpoch 14/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.8216 - loss: 0.4807 - val_accuracy: 0.7996 - val_loss: 0.5613 - learning_rate: 0.0010\nEpoch 15/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.8408 - loss: 0.4449 - val_accuracy: 0.7918 - val_loss: 0.6068 - learning_rate: 0.0010\nEpoch 16/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.8429 - loss: 0.4258 - val_accuracy: 0.8063 - val_loss: 0.5539 - learning_rate: 0.0010\nEpoch 17/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.8532 - loss: 0.4029 - val_accuracy: 0.8175 - val_loss: 0.5284 - learning_rate: 0.0010\nEpoch 18/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.8619 - loss: 0.3868 - val_accuracy: 0.8309 - val_loss: 0.4898 - learning_rate: 0.0010\nEpoch 19/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.8691 - loss: 0.3598 - val_accuracy: 0.8293 - val_loss: 0.5109 - learning_rate: 0.0010\nEpoch 20/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - accuracy: 0.8782 - loss: 0.3387 - val_accuracy: 0.8365 - val_loss: 0.5007 - learning_rate: 0.0010\nEpoch 21/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.8838 - loss: 0.3191 - val_accuracy: 0.8362 - val_loss: 0.4973 - learning_rate: 0.0010\nEpoch 22/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.8947 - loss: 0.2840 - val_accuracy: 0.8313 - val_loss: 0.5185 - learning_rate: 0.0010\nEpoch 23/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.8961 - loss: 0.2948 - val_accuracy: 0.8385 - val_loss: 0.4914 - learning_rate: 0.0010\nEpoch 24/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.9267 - loss: 0.2097 - val_accuracy: 0.8742 - val_loss: 0.3911 - learning_rate: 2.0000e-04\nEpoch 25/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.9463 - loss: 0.1600 - val_accuracy: 0.8753 - val_loss: 0.3887 - learning_rate: 2.0000e-04\nEpoch 26/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.9558 - loss: 0.1389 - val_accuracy: 0.8811 - val_loss: 0.3831 - learning_rate: 2.0000e-04\nEpoch 27/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.9576 - loss: 0.1297 - val_accuracy: 0.8818 - val_loss: 0.3832 - learning_rate: 2.0000e-04\nEpoch 28/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.9627 - loss: 0.1218 - val_accuracy: 0.8787 - val_loss: 0.4010 - learning_rate: 2.0000e-04\nEpoch 29/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.9645 - loss: 0.1111 - val_accuracy: 0.8773 - val_loss: 0.4118 - learning_rate: 2.0000e-04\nEpoch 30/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.9671 - loss: 0.1059 - val_accuracy: 0.8803 - val_loss: 0.4040 - learning_rate: 2.0000e-04\nValidation accuracy for fold 1: 0.8803\nTraining fold 2...\nEpoch 1/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - accuracy: 0.4009 - loss: 1.6588 - val_accuracy: 0.5400 - val_loss: 1.2566 - learning_rate: 0.0010\nEpoch 2/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.5431 - loss: 1.2655 - val_accuracy: 0.5690 - val_loss: 1.1483 - learning_rate: 0.0010\nEpoch 3/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - accuracy: 0.6017 - loss: 1.0836 - val_accuracy: 0.6171 - val_loss: 1.0584 - learning_rate: 0.0010\nEpoch 4/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.6498 - loss: 0.9543 - val_accuracy: 0.6428 - val_loss: 0.9679 - learning_rate: 0.0010\nEpoch 5/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.6614 - loss: 0.9002 - val_accuracy: 0.6629 - val_loss: 0.8966 - learning_rate: 0.0010\nEpoch 6/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.6922 - loss: 0.8287 - val_accuracy: 0.7062 - val_loss: 0.7848 - learning_rate: 0.0010\nEpoch 7/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - accuracy: 0.7187 - loss: 0.7645 - val_accuracy: 0.7080 - val_loss: 0.8073 - learning_rate: 0.0010\nEpoch 8/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.7405 - loss: 0.7137 - val_accuracy: 0.7433 - val_loss: 0.7062 - learning_rate: 0.0010\nEpoch 9/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.7538 - loss: 0.6675 - val_accuracy: 0.7424 - val_loss: 0.6930 - learning_rate: 0.0010\nEpoch 10/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.7753 - loss: 0.6053 - val_accuracy: 0.7455 - val_loss: 0.6840 - learning_rate: 0.0010\nEpoch 11/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.7810 - loss: 0.5936 - val_accuracy: 0.7547 - val_loss: 0.6659 - learning_rate: 0.0010\nEpoch 12/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.8028 - loss: 0.5457 - val_accuracy: 0.7714 - val_loss: 0.6268 - learning_rate: 0.0010\nEpoch 13/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.8217 - loss: 0.4957 - val_accuracy: 0.7933 - val_loss: 0.5738 - learning_rate: 0.0010\nEpoch 14/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.8353 - loss: 0.4567 - val_accuracy: 0.7802 - val_loss: 0.6159 - learning_rate: 0.0010\nEpoch 15/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.8367 - loss: 0.4487 - val_accuracy: 0.7994 - val_loss: 0.5793 - learning_rate: 0.0010\nEpoch 16/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.8552 - loss: 0.4083 - val_accuracy: 0.7954 - val_loss: 0.5680 - learning_rate: 0.0010\nEpoch 17/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - accuracy: 0.8542 - loss: 0.3973 - val_accuracy: 0.8248 - val_loss: 0.4990 - learning_rate: 0.0010\nEpoch 18/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.8741 - loss: 0.3547 - val_accuracy: 0.8132 - val_loss: 0.5412 - learning_rate: 0.0010\nEpoch 19/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.8655 - loss: 0.3768 - val_accuracy: 0.8233 - val_loss: 0.5098 - learning_rate: 0.0010\nEpoch 20/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.8883 - loss: 0.3160 - val_accuracy: 0.8304 - val_loss: 0.4945 - learning_rate: 0.0010\nEpoch 21/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - accuracy: 0.8947 - loss: 0.2983 - val_accuracy: 0.8320 - val_loss: 0.4868 - learning_rate: 0.0010\nEpoch 22/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.8961 - loss: 0.2920 - val_accuracy: 0.8396 - val_loss: 0.4841 - learning_rate: 0.0010\nEpoch 23/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.9060 - loss: 0.2614 - val_accuracy: 0.8407 - val_loss: 0.4940 - learning_rate: 0.0010\nEpoch 24/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.9091 - loss: 0.2511 - val_accuracy: 0.8519 - val_loss: 0.4692 - learning_rate: 0.0010\nEpoch 25/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.9128 - loss: 0.2434 - val_accuracy: 0.8358 - val_loss: 0.5065 - learning_rate: 0.0010\nEpoch 26/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - accuracy: 0.9188 - loss: 0.2235 - val_accuracy: 0.8382 - val_loss: 0.5161 - learning_rate: 0.0010\nEpoch 27/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.9275 - loss: 0.2084 - val_accuracy: 0.8467 - val_loss: 0.4973 - learning_rate: 0.0010\nEpoch 28/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.9273 - loss: 0.2048 - val_accuracy: 0.8385 - val_loss: 0.5067 - learning_rate: 0.0010\nEpoch 29/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.9361 - loss: 0.1810 - val_accuracy: 0.8530 - val_loss: 0.4872 - learning_rate: 0.0010\nEpoch 30/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.9522 - loss: 0.1408 - val_accuracy: 0.8751 - val_loss: 0.4175 - learning_rate: 2.0000e-04\nValidation accuracy for fold 2: 0.8751\nTraining fold 3...\nEpoch 1/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - accuracy: 0.3967 - loss: 1.6621 - val_accuracy: 0.5378 - val_loss: 1.2843 - learning_rate: 0.0010\nEpoch 2/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.5576 - loss: 1.2214 - val_accuracy: 0.5724 - val_loss: 1.1349 - learning_rate: 0.0010\nEpoch 3/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.5949 - loss: 1.0919 - val_accuracy: 0.6182 - val_loss: 1.0166 - learning_rate: 0.0010\nEpoch 4/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.6382 - loss: 0.9733 - val_accuracy: 0.6501 - val_loss: 0.9357 - learning_rate: 0.0010\nEpoch 5/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.6707 - loss: 0.8886 - val_accuracy: 0.5927 - val_loss: 1.2208 - learning_rate: 0.0010\nEpoch 6/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.6970 - loss: 0.8258 - val_accuracy: 0.6852 - val_loss: 0.8472 - learning_rate: 0.0010\nEpoch 7/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.7187 - loss: 0.7631 - val_accuracy: 0.6883 - val_loss: 0.8475 - learning_rate: 0.0010\nEpoch 8/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.7328 - loss: 0.7076 - val_accuracy: 0.7207 - val_loss: 0.7754 - learning_rate: 0.0010\nEpoch 9/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.7545 - loss: 0.6584 - val_accuracy: 0.7426 - val_loss: 0.7054 - learning_rate: 0.0010\nEpoch 10/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.7821 - loss: 0.6053 - val_accuracy: 0.7426 - val_loss: 0.7133 - learning_rate: 0.0010\nEpoch 11/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.7912 - loss: 0.5654 - val_accuracy: 0.7623 - val_loss: 0.6817 - learning_rate: 0.0010\nEpoch 12/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.8000 - loss: 0.5508 - val_accuracy: 0.7609 - val_loss: 0.6348 - learning_rate: 0.0010\nEpoch 13/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.8250 - loss: 0.4824 - val_accuracy: 0.7956 - val_loss: 0.5788 - learning_rate: 0.0010\nEpoch 14/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.8355 - loss: 0.4506 - val_accuracy: 0.8000 - val_loss: 0.5597 - learning_rate: 0.0010\nEpoch 15/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.8426 - loss: 0.4399 - val_accuracy: 0.7978 - val_loss: 0.5828 - learning_rate: 0.0010\nEpoch 16/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.8548 - loss: 0.4050 - val_accuracy: 0.8184 - val_loss: 0.5395 - learning_rate: 0.0010\nEpoch 17/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 17ms/step - accuracy: 0.8673 - loss: 0.3734 - val_accuracy: 0.8052 - val_loss: 0.5488 - learning_rate: 0.0010\nEpoch 18/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.8732 - loss: 0.3521 - val_accuracy: 0.8148 - val_loss: 0.5482 - learning_rate: 0.0010\nEpoch 19/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.8788 - loss: 0.3352 - val_accuracy: 0.8304 - val_loss: 0.5125 - learning_rate: 0.0010\nEpoch 20/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.8832 - loss: 0.3244 - val_accuracy: 0.8262 - val_loss: 0.5221 - learning_rate: 0.0010\nEpoch 21/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.8937 - loss: 0.2885 - val_accuracy: 0.8224 - val_loss: 0.5445 - learning_rate: 0.0010\nEpoch 22/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.9033 - loss: 0.2730 - val_accuracy: 0.8275 - val_loss: 0.5264 - learning_rate: 0.0010\nEpoch 23/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.9024 - loss: 0.2693 - val_accuracy: 0.8322 - val_loss: 0.5452 - learning_rate: 0.0010\nEpoch 24/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.9077 - loss: 0.2586 - val_accuracy: 0.8336 - val_loss: 0.5350 - learning_rate: 0.0010\nEpoch 25/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.9364 - loss: 0.1863 - val_accuracy: 0.8711 - val_loss: 0.4265 - learning_rate: 2.0000e-04\nEpoch 26/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.9571 - loss: 0.1310 - val_accuracy: 0.8738 - val_loss: 0.4327 - learning_rate: 2.0000e-04\nEpoch 27/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.9630 - loss: 0.1157 - val_accuracy: 0.8756 - val_loss: 0.4379 - learning_rate: 2.0000e-04\nEpoch 28/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.9652 - loss: 0.1060 - val_accuracy: 0.8778 - val_loss: 0.4300 - learning_rate: 2.0000e-04\nEpoch 29/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.9674 - loss: 0.1026 - val_accuracy: 0.8765 - val_loss: 0.4329 - learning_rate: 2.0000e-04\nEpoch 30/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.9687 - loss: 0.0951 - val_accuracy: 0.8744 - val_loss: 0.4336 - learning_rate: 2.0000e-04\nValidation accuracy for fold 3: 0.8744\nTraining fold 4...\nEpoch 1/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - accuracy: 0.3943 - loss: 1.6379 - val_accuracy: 0.5483 - val_loss: 1.2467 - learning_rate: 0.0010\nEpoch 2/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - accuracy: 0.5440 - loss: 1.2215 - val_accuracy: 0.5655 - val_loss: 1.1894 - learning_rate: 0.0010\nEpoch 3/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - accuracy: 0.6050 - loss: 1.0655 - val_accuracy: 0.5934 - val_loss: 1.1435 - learning_rate: 0.0010\nEpoch 4/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.6441 - loss: 0.9513 - val_accuracy: 0.6584 - val_loss: 0.9165 - learning_rate: 0.0010\nEpoch 5/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - accuracy: 0.6684 - loss: 0.8786 - val_accuracy: 0.6888 - val_loss: 0.8738 - learning_rate: 0.0010\nEpoch 6/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.7018 - loss: 0.8019 - val_accuracy: 0.6879 - val_loss: 0.8200 - learning_rate: 0.0010\nEpoch 7/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - accuracy: 0.7198 - loss: 0.7493 - val_accuracy: 0.7212 - val_loss: 0.7488 - learning_rate: 0.0010\nEpoch 8/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.7372 - loss: 0.7191 - val_accuracy: 0.7223 - val_loss: 0.7603 - learning_rate: 0.0010\nEpoch 9/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.7650 - loss: 0.6484 - val_accuracy: 0.7359 - val_loss: 0.7093 - learning_rate: 0.0010\nEpoch 10/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - accuracy: 0.7681 - loss: 0.6206 - val_accuracy: 0.7744 - val_loss: 0.6221 - learning_rate: 0.0010\nEpoch 11/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - accuracy: 0.8007 - loss: 0.5513 - val_accuracy: 0.7759 - val_loss: 0.6181 - learning_rate: 0.0010\nEpoch 12/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.8080 - loss: 0.5206 - val_accuracy: 0.7723 - val_loss: 0.6450 - learning_rate: 0.0010\nEpoch 13/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.8209 - loss: 0.4889 - val_accuracy: 0.7862 - val_loss: 0.6039 - learning_rate: 0.0010\nEpoch 14/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.8306 - loss: 0.4644 - val_accuracy: 0.7922 - val_loss: 0.5941 - learning_rate: 0.0010\nEpoch 15/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.8467 - loss: 0.4198 - val_accuracy: 0.7893 - val_loss: 0.5964 - learning_rate: 0.0010\nEpoch 16/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - accuracy: 0.8490 - loss: 0.4112 - val_accuracy: 0.7956 - val_loss: 0.5930 - learning_rate: 0.0010\nEpoch 17/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.8549 - loss: 0.3942 - val_accuracy: 0.8228 - val_loss: 0.5090 - learning_rate: 0.0010\nEpoch 18/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.8691 - loss: 0.3667 - val_accuracy: 0.8094 - val_loss: 0.5503 - learning_rate: 0.0010\nEpoch 19/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.8708 - loss: 0.3517 - val_accuracy: 0.8029 - val_loss: 0.5778 - learning_rate: 0.0010\nEpoch 20/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - accuracy: 0.8860 - loss: 0.3230 - val_accuracy: 0.8242 - val_loss: 0.5210 - learning_rate: 0.0010\nEpoch 21/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - accuracy: 0.8949 - loss: 0.2989 - val_accuracy: 0.8097 - val_loss: 0.5848 - learning_rate: 0.0010\nEpoch 22/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.9006 - loss: 0.2817 - val_accuracy: 0.8282 - val_loss: 0.5436 - learning_rate: 0.0010\nEpoch 23/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.9230 - loss: 0.2231 - val_accuracy: 0.8660 - val_loss: 0.4199 - learning_rate: 2.0000e-04\nEpoch 24/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.9487 - loss: 0.1600 - val_accuracy: 0.8702 - val_loss: 0.4150 - learning_rate: 2.0000e-04\nEpoch 25/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - accuracy: 0.9505 - loss: 0.1439 - val_accuracy: 0.8724 - val_loss: 0.4105 - learning_rate: 2.0000e-04\nEpoch 26/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.9536 - loss: 0.1310 - val_accuracy: 0.8722 - val_loss: 0.4035 - learning_rate: 2.0000e-04\nEpoch 27/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.9578 - loss: 0.1278 - val_accuracy: 0.8735 - val_loss: 0.4055 - learning_rate: 2.0000e-04\nEpoch 28/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - accuracy: 0.9580 - loss: 0.1244 - val_accuracy: 0.8760 - val_loss: 0.4126 - learning_rate: 2.0000e-04\nEpoch 29/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.9616 - loss: 0.1085 - val_accuracy: 0.8809 - val_loss: 0.4135 - learning_rate: 2.0000e-04\nEpoch 30/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.9658 - loss: 0.1039 - val_accuracy: 0.8798 - val_loss: 0.4131 - learning_rate: 2.0000e-04\nValidation accuracy for fold 4: 0.8798\nTraining fold 5...\nEpoch 1/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - accuracy: 0.3934 - loss: 1.6864 - val_accuracy: 0.5136 - val_loss: 1.3455 - learning_rate: 0.0010\nEpoch 2/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.5335 - loss: 1.2731 - val_accuracy: 0.5943 - val_loss: 1.1269 - learning_rate: 0.0010\nEpoch 3/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.5927 - loss: 1.1150 - val_accuracy: 0.6285 - val_loss: 0.9769 - learning_rate: 0.0010\nEpoch 4/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.6361 - loss: 0.9939 - val_accuracy: 0.6714 - val_loss: 0.8807 - learning_rate: 0.0010\nEpoch 5/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.6604 - loss: 0.9144 - val_accuracy: 0.6224 - val_loss: 1.0194 - learning_rate: 0.0010\nEpoch 6/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.6910 - loss: 0.8401 - val_accuracy: 0.7078 - val_loss: 0.7942 - learning_rate: 0.0010\nEpoch 7/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - accuracy: 0.7168 - loss: 0.7616 - val_accuracy: 0.6957 - val_loss: 0.8303 - learning_rate: 0.0010\nEpoch 8/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.7314 - loss: 0.7268 - val_accuracy: 0.7399 - val_loss: 0.7028 - learning_rate: 0.0010\nEpoch 9/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.7551 - loss: 0.6532 - val_accuracy: 0.7630 - val_loss: 0.6560 - learning_rate: 0.0010\nEpoch 10/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.7775 - loss: 0.6109 - val_accuracy: 0.7739 - val_loss: 0.6275 - learning_rate: 0.0010\nEpoch 11/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.8717 - loss: 0.3572 - val_accuracy: 0.8130 - val_loss: 0.5452 - learning_rate: 0.0010\nEpoch 19/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.8753 - loss: 0.3416 - val_accuracy: 0.8199 - val_loss: 0.5109 - learning_rate: 0.0010\nEpoch 20/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.8909 - loss: 0.3064 - val_accuracy: 0.8396 - val_loss: 0.4656 - learning_rate: 0.0010\nEpoch 21/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.8958 - loss: 0.2952 - val_accuracy: 0.8300 - val_loss: 0.4964 - learning_rate: 0.0010\nEpoch 22/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.8954 - loss: 0.2857 - val_accuracy: 0.8396 - val_loss: 0.4829 - learning_rate: 0.0010\nEpoch 23/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.9022 - loss: 0.2707 - val_accuracy: 0.8315 - val_loss: 0.5138 - learning_rate: 0.0010\nEpoch 24/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.9139 - loss: 0.2420 - val_accuracy: 0.8382 - val_loss: 0.4965 - learning_rate: 0.0010\nEpoch 25/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.9229 - loss: 0.2157 - val_accuracy: 0.8432 - val_loss: 0.4857 - learning_rate: 0.0010\nEpoch 26/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.9439 - loss: 0.1648 - val_accuracy: 0.8729 - val_loss: 0.4061 - learning_rate: 2.0000e-04\nEpoch 27/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.9616 - loss: 0.1161 - val_accuracy: 0.8749 - val_loss: 0.4071 - learning_rate: 2.0000e-04\nEpoch 28/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.9701 - loss: 0.0985 - val_accuracy: 0.8765 - val_loss: 0.4034 - learning_rate: 2.0000e-04\nEpoch 29/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.9693 - loss: 0.0959 - val_accuracy: 0.8803 - val_loss: 0.4032 - learning_rate: 2.0000e-04\nEpoch 30/30\n\u001b[1m560/560\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.9704 - loss: 0.0890 - val_accuracy: 0.8807 - val_loss: 0.4147 - learning_rate: 2.0000e-04\nValidation accuracy for fold 5: 0.8807\n\nAverage validation accuracy across all folds: 0.8781\n","output_type":"stream"}],"execution_count":10}]}